# -*- coding: utf-8 -*-
"""MichaelSemanFinalProjectAmex.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PUadSWyrTIFkg4Gy05_d-wPycLPAl67k

# Data Wrangling Final Project
https://www.kaggle.com/datasets/hotsonhonet/amex-competition/versions/1?resource=download

We are working with the 2021 amex competition dataset.

We are trying to predict whether or not they will default on their credit card.

Also for reference, I'm using this template for this project. https://colab.research.google.com/drive/1agNKQQZL-zgGspPtBW5ZLiM8RHRmyJTH?usp=sharing

My Numbering does not match the final project module's numbering.

# (1.) Load Libraries and Dataset
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
# %matplotlib inline

df = pd.read_csv("https://raw.githubusercontent.com/michaelseman/mydatasets/main/amex.csv")

"""# (2.) Structural Investigation of data"""

df.head()

# we can drop first two columns for sure. customer_id and name

df.info()

# data types look ok.  definitely some nulls to deal with
# percent credit limit usage as an int is interesting

df.nunique()

# i was worried about occupation type, but only 19 values, doesnt seem too bad

df.gender.value_counts()

df.shape

df.corr()

"""## Thoughts: correlation"""

# there is a really high and expected correlation between default in last 6 months and prev_defaults
# these also have the higest correlation with our outcome variable
# total family members and number of children very highly correlated... more or less measuring the same thing.... we should probably eliminate 1
# credit limit and annual income almost a 1 to 1 correlation, we will have to eliminate 1 for sure

df.describe()

# look at the max value for number of days worked... we have some outliers to treat later
365252/365

"""## checking datatypes"""

df.dtypes

# [no_of_children, no_of_days_employed, total_family_members] should be int64 not float
# migrant_worker could be a category
# we need to take care of nulls first before converting datatypes

"""## checking for nulls"""

# Check for nulls --- you do NOT want nulls when you train
df.isnull().sum()

"""### dropping customer_id, name"""

df.drop(['customer_id','name'], axis=1,inplace=True)

"""## checking distribution of target variable"""

df.credit_card_default.value_counts(normalize=True)

"""## Splitting data into numeric and categorical"""

# Split the data into numeric and categorical lists and dataframes
numerics = ['int16','int32','int64','float64']
catDF = df.select_dtypes(exclude=numerics)
numDF = df.select_dtypes(include=numerics)
catDF.head()

numDF.head()

"""# splitting X and y"""

# This is how you merge the datasets back together:
# Merge back into a single df
# preparing the X Variables  (Don't forget ot remove the target!!)
X = pd.concat([catDF,numDF],axis=1)
print(X.shape)

# This is how you extract the target variable
y = X['credit_card_default']
X.drop(['credit_card_default'],axis=1,inplace=True)
print(df.shape)
print(X.shape)
print(y.shape)

# dropping target from numDF
numDF.drop(['credit_card_default'],axis=1,inplace=True)

numDF.head()

"""# Creating a baseline model
So we are going to run our DF through a model, having only eliminated the name and customerid columns

we are going to put 0 in place of all the nulls
"""

# Split the data into test, train, validation sets... 80/20
from sklearn.model_selection import train_test_split
# This gives the 80/20 train test split
df_train_full, df_test = train_test_split(df, test_size=0.2, random_state=11, stratify=df.credit_card_default)

len(df_train_full), len(df_test)
# Replace nulls with 0's - these are pandas dataframes
df_train_full = df_train_full.fillna(0)

df_test = df_test.fillna(0)
len(df_train_full),len(df_test)

df_train_full.credit_card_default.value_counts(normalize=True)

#Split the y out into train/test/splits... these are numpy ndarrays ... msrp is your target variables
# Replace with your target variable!!!  
y_train = (df_train_full.credit_card_default).values
y_test = (df_test.credit_card_default).values
del df_train_full['credit_card_default']
del df_test['credit_card_default']

len(y_train),len(y_test)

# Encode the data
# Convert these data frames into a LIST of DICTIONARIES (each element in the list is a dictionary (the record))
dict_train = df_train_full.to_dict(orient='records')
dict_test = df_test.to_dict(orient='records')

dict_test[1]

# Convert the LIST OF DICTIONARIES into a Feature Matrix (does all of the encoding)
from sklearn.feature_extraction import DictVectorizer
 
dv = DictVectorizer(sparse=False)
 
X_train = dv.fit_transform(dict_train)
X_test = dv.transform(dict_test)
features = dv.get_feature_names_out()

features

X_train.shape

"""## Harness"""

# Compare Algorithms
from sklearn.metrics import roc_auc_score
from time import time
from sklearn.metrics import explained_variance_score,mean_absolute_error,r2_score
from pandas import read_csv
from matplotlib import pyplot
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from xgboost import XGBClassifier
models = []
models.append(('LR', LogisticRegression(solver='liblinear')))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('XGB', XGBClassifier()))
# models.append(('SVM', SVC(gamma='auto')))
# evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
    start = time()
    kfold = KFold(n_splits=10, random_state=7, shuffle=True)
    model.fit(X_train, y_train)
    train_time = time() - start
    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)
    predict_time = time()-start 
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    #y_pred = model.predict_proba(X_train)[:, 1]
    #auc = roc_auc_score(y_train, y_pred)
    print(msg)
    print("Score for each of the 10 K-fold tests: ",cv_results)
    print(model)
    print("\tTraining time: %0.3fs" % train_time)
    print("\tPrediction time: %0.3fs" % predict_time)
    #y_pred = model.predict(X_test)
    #print("\tExplained variance:", explained_variance_score(y_test, y_pred))
    print()
    
    
    
# boxplot algorithm comparison
fig = pyplot.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
pyplot.show()

# SVC was taking an absurdly long time, so we left it out

"""## lets look deeper into the LDA model
this is one of our best performing models, so this will be our baseline

"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
model = LinearDiscriminantAnalysis()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
confusion_matrix(y_test, y_pred)

from sklearn.metrics import classification_report
print (classification_report (y_test, y_pred))

"""### Thoughts: baseline model
- So we were able to get around 98% accuracy
- The dataset is heavily weight, with around 91% of rows not defaulting
- So we did better than just predicting NO default for every outcome
- the 69% recall score is the one we want to improve, we want to be able to indentify those who will default.

# (3.)  Qualitative Investigation of the Data

## Duplicates
"""

# Duplicates in the Columns?
df.duplicated()
df.duplicated().sum()

# Duplicated Rows?
df[df.duplicated()]

"""## Missing Values per Sample"""

# b) Missing Values per Sample (Big Holes)


plt.figure(figsize=(15, 8))
sns.set_style('whitegrid')

g = sns.heatmap(X.isnull(), cbar=False, cmap='viridis')

g.set_xlabel('Column Number')
g.set_ylabel('Sample Number')

X.isna().sum()

!pip install missingno
import missingno as msno
msno.matrix(X, labels=True, sort='descending', color=(0.27, 0.52, 1.0));

X.shape

# Drop rows that are 20% or more empty (you set the threshold)
X = X.dropna(thresh=X.shape[1] * 0.80, axis=0).reset_index(drop=True)
X.shape

"""## Missing Values per Feature"""

# c) Missing Values per Feature (Big Holes)
X.isna().mean().sort_values().plot(
    kind="bar", figsize=(15, 4),
    title="Percentage of missing values per feature",

    ylabel="Ratio of missing values per feature");

X.shape

"""### dropping number_of_children"""

# as we noted before, no of children and total family members are highly correlated... given the amount of nulls for number of children, lets drop this column
# dropping columns missing 15% of their data (nulls)
X = X.dropna(thresh=X.shape[0] * 0.985, axis=1)
X.shape

"""## Impute Values
This will fill all the small holes / nulls with the mean
"""

X.isna().sum()

column_list_num=['no_of_days_employed','total_family_members','yearly_debt_payments', 'credit_score']
column_list_cat=['owns_car','migrant_worker']

# imputing mean for numeric
for column in column_list_num:
  X[column] = X[column].fillna(X[column].mean())

# imputing mode for categorical
for column in column_list_cat:
  X[column] = X[column].fillna(X[column].mode().iloc[0])

X.isna().sum()

"""### converting datatypes"""

# convert columns to int

X["no_of_days_employed"] = X["no_of_days_employed"].astype(int)
X["total_family_members"] = X["total_family_members"].astype(int)
X["migrant_worker"] = X["migrant_worker"].astype(int)

X.dtypes

"""## Plotting all Numeric Features"""

# Split the data into numeric and categorical lists and dataframes
numerics = ['int16','int32','int64','float64']
catDF = X.select_dtypes(exclude=numerics)
numDF = X.select_dtypes(include=numerics)
catDF.head()

numDF.head()

# Plot all numeric features:
numDF.plot(lw=0, marker=".", subplots=True, layout=(-1, 4), # -1 means use as many as possible
          figsize=(15, 30), markersize=1);

numDF.describe()

"""## Plotting Categorical Features"""

# Create figure object with 3 subplots
fig, axes = plt.subplots(ncols=1, nrows=3, figsize=(12, 8))


# Loop through features and put each subplot on a matplotlib axis object
for col, ax in zip(catDF.columns, axes.ravel()):

    # Selects one single feature and counts number of occurrences per unique value
    catDF[col].value_counts().plot(

        # Plots this information in a figure with log-scaled y-axis
        logy=True, title=col, lw=0, marker=".", ax=ax)
    
plt.tight_layout();

# I wonder if we need to do something about that 1 random row, that has gender as XNA

"""## Pandas Profiling"""

!pip install pandas-profiling==3.2.0
!pip install markupsafe==2.0.1
# https://pypi.org/project/pandas-profiling/3.1.0/#history

import pandas as pd
from pandas_profiling import ProfileReport

report = ProfileReport(df)
report

"""### Thoughts:
- So we dropped number_of_children as it had the most missing values and was highly correlated with family size.
- we also imputed the mean to eliminate all nulls.  This was an easy decision as this dataset had very few nulls to begin with
"""



"""# (4.) Content Investigation of the Data

## Feature Distribution
"""

# Commented out IPython magic to ensure Python compatibility.
# # Feature Distribution
# %%time
# # Plots the histogram for each numerical feature in a separate subplot
# numDF.hist(bins=25, figsize=(15, 25), layout=(-1, 5),
# edgecolor="black")
# plt.tight_layout();

# Collects for each categorical feature the most frequent entry
most_frequent_entry = X.select_dtypes(include='object').mode()
# Computes the ratio of singular value content for each feature
df_freq = (X == most_frequent_entry.squeeze()).mean().sort_values(ascending=False)
# Show the top 5 features with the highest ratio of singular value content
display(df_freq.head())
# Visualize the 'df_freq' table
df_freq.plot.bar(figsize=(15, 4))

"""## Feature Relationship"""

# Feature Relationships
# Evaluate but remember to consider multicollinearity

# Computes feature correlation
df_corr = X.corr(method="pearson")

# Create labels for the correlation matrix
labels = np.where(np.abs(df_corr)>0.75, "S",
                  np.where(np.abs(df_corr)>0.5, "M",
                           np.where(np.abs(df_corr)>0.25, "W", "")))

# Plot correlation matrix
plt.figure(figsize=(15, 15))
sns.heatmap(df_corr, mask=np.eye(len(df_corr)), square=True,
            center=0, annot=labels, fmt='', linewidths=.5,
            cmap="vlag", cbar_kws={"shrink": 0.8});

"""## Feature Patterns"""

X.plot(lw=0, marker=".", subplots=True, layout=(-1, 2), markersize=0.1,
figsize=(25, 16));

"""## Analyze Continuous Features"""

# Creates mask to identify numerical features with more or less than 25 unique features
cols_continuous = df.select_dtypes(include="number").nunique() >= 25

# Commented out IPython magic to ensure Python compatibility.
# # Analyze Continuous Features
# %%time
# # Create a new dataframe which only contains the continuous features
# df_continuous = df[cols_continuous[cols_continuous].index]
# df_continuous.shape
# sns.pairplot(df_continuous, height=1.5, plot_kws={"s": 2, "alpha":0.2});



"""## Discrete and Ordinal Features"""

# Discrete and Ordinal Features
# Create a new dataframe which doesn't contain the numerical continuous features
df_discrete = df[cols_continuous[~cols_continuous].index]
df_discrete.shape

# Establish number of columns and rows needed to plot all features
n_cols = 5
n_elements = len(df_discrete.columns)
n_rows = np.ceil(n_elements / n_cols).astype("int")
# Specify y_value to spread data (ideally a continuous feature)
y_value = df["age"]
# Create figure object with as many rows and columns as needed
fig, axes = plt.subplots(ncols=n_cols, nrows=n_rows, figsize=(15, n_rows * 2.5))
# Loop through features and put each subplot on a matplotlib axis object
for col, ax in zip(df_discrete.columns, axes.ravel()):
  sns.stripplot(data=df, x=col, y=y_value, ax=ax, palette="tab10",size=1, alpha=0.5)
  plt.tight_layout();

"""### Thoughts:
- credit limit used % and credit score both do not seem to be normally distributed.
- net yearly income and credit limit are HIGHLY correlated.  One will have to be removed in feature selection.
- the last two columns, previous defaults and default in last 6 months are obviously the most highly correlated with our outcome variable
- as credit limit used % goes up you are more likely to default, and credit score go up you are less likely to default.  These correlations are decently strong... around .3/.4
- we also see that theres a negative correlation between number of family members and numbers of days at job.

# (5.) Outliers
"""

X.isnull().sum()

# Split the data into numeric and categorical lists and dataframes
numerics = ['int16','int32','int64','float64']
catDF = X.select_dtypes(exclude=numerics)
numDF = X.select_dtypes(include=numerics)
catDF.head()

numDF.head()

catDF['migrant_worker']=numDF['migrant_worker']
numDF.drop('migrant_worker',axis=1,inplace=True)
save_for_later=numDF[['prev_defaults', 'default_in_last_6months']].copy()
numDF = numDF.drop(columns=['prev_defaults', 'default_in_last_6months'])

for column in numDF:  
  print(df[column].describe())
  q1 = df[column].quantile(.25)
  q3 = df[column].quantile(.75)
  IQR = q3-q1
  lower_lim = q1 - 1.5 * IQR
  upper_lim = q3 +1.5 * IQR
  print(column)
  print(lower_lim)
  print(upper_lim)

for column in numDF:
  sns.boxplot(x=numDF[column])
  plt.show()

"""## Winsorizing"""

from scipy.stats.mstats import winsorize

# had some issues fixing this in the loop so I had to do individually with a lower threshold
numDF['no_of_days_employed'] = winsorize(numDF['no_of_days_employed'], (0.15, 0.2))

# for some reason, when I did this in the loop it was elimiating families with 4 members down to three, so I did that here
numDF['total_family_members']=winsorize(numDF['total_family_members'], (0.05, 0.05))

# holding these columns to add back in later.
save_for_later['total_family_members']=numDF['total_family_members']

save_for_later.head()

numDF.drop('total_family_members',axis=1,inplace=True)

# winsorizing in a loop
for column in numDF:
  numDF[column] = winsorize(numDF[column], (0.1, 0.1))

# checking results
for column in numDF:
  sns.boxplot(x=numDF[column])
  plt.show()

numDF = numDF.merge(save_for_later, how='outer', left_index=True, right_index=True)

numDF.head()

"""### Thoughts:
The main column that had EXTREME outliers was no of days employed.  It had an insane outlier of around 300k days... for some reason I could not get this to be removed with winsorization, just .1 like I did for everything else.  Also had an issue with winsorizing the number of family members, so I had to adjust that.

I was hoping looking at the outliers/box plot for credit limit and net yearly income might give me some insight on what column to drop, but they look nearly identical.

I should have left the code, but some of the values that I thought logarithmic transformation would work better for, still ended up with a lot of outliers, so I ditched this approach and winsorized.  Like I said, I should have left the code though.

# (.6) EDA

## Univariate Analysis
"""

# getting my X values back together
X = pd.concat([catDF, numDF],axis=1)

# adding y to catDF in order to visualize data easier
catDF=pd.concat([catDF,y],axis=1)

fig, axs = plt.subplots(nrows=1, ncols=len(catDF.columns), figsize=(20,5))
for idx, column in enumerate(catDF.columns):
    sns.countplot(x=column, data=catDF, ax=axs[idx])
    
plt.tight_layout()
plt.show()

catDF.credit_card_default.value_counts()

sns.set(rc={'figure.figsize':(20,10)})
sns.countplot(x="occupation_type", data=catDF)
plt.xticks(rotation=45)

plt.show()

catDF.occupation_type.value_counts(normalize=True)

"""### Thoughts:
1. First we are dealing with a highly imbalanced dataset. only ~3.7k out of ~45k default
2. Most of the records are women, who don't own a car but do own a house.
3. It will be interesting to see what we find out from feature importance, because in terms of job, the majority of our records are unknown (31.4%)... we might need to go back later and drop this.

## Bivariate Analysis
"""

fig, axs = plt.subplots(nrows=1, ncols=len(catDF.columns), figsize=(20,5))

for idx, column in enumerate(catDF.columns):
    sns.set(rc={'figure.figsize':(15,10)})
    edu = sns.countplot(x=column, hue='credit_card_default', data=catDF, ax=axs[idx])
    
plt.tight_layout()
plt.show()

sns.set(rc={'figure.figsize':(20,10)})
sns.countplot(x="occupation_type", hue='credit_card_default',data=catDF)
plt.xticks(rotation=45)

plt.show()

pd.crosstab(catDF.occupation_type,catDF.credit_card_default,normalize='index',margins=True)

result = pd.crosstab(catDF.occupation_type, catDF.credit_card_default, normalize='index', margins=True)
result_sorted = result.sort_values(by=1, ascending=False)
result_rounded = result_sorted.round(3)

print(result_rounded)

for column in catDF.columns:
    result = pd.crosstab(catDF[column], catDF.credit_card_default, normalize='index', margins=True)
    
    result_rounded = result.round(3)

    print(f"\nColumn: {column}\n{result_rounded}")

# quick check into occupation type and migrant worker
pd.crosstab(catDF.occupation_type, catDF.migrant_worker, margins=True)

"""### Thoughts:
- men are more likely to default
- owning a house seems to have no impact on defaulting
- I thought migrant workers might have been a break away column or subset of occupation.  But it seems to be its own category.  I was hoping that migrant workers would take up a lot of the unknown occupation values, but that is not the case.
- low skilled laborers are most likely occupation to default
- migrant workers and non car owners are slightly more likely to default

## Multivariate
"""

sns.set(rc={'figure.figsize':(30,10)})
sns.set_context("talk", font_scale=0.7)
sns.heatmap(df.iloc[:,1:].corr(method='spearman'), cmap='rainbow_r', annot=True)

"""### Fixing Gender Issue"""

# i'm eliminating the 1 row where the gender is XNA from both sets of data
X = X[X['gender'] != 'XNA']
y = y.loc[X.index]

X['gender'].value_counts()

catDF.head()

catDF.columns

catDF.dtypes

catDF.credit_card_default.value_counts()

selected_features = catDF.columns
# Create a figure with 5 subplots
fig, axes = plt.subplots(ncols=1, nrows=5, figsize=(26, 16))
# Loop through these features and plot entries from each feature against `Latitude`
for col, ax in zip(selected_features, axes.ravel()):
  sns.violinplot(data=X, x=col, y=X["age"],palette="Set2",split=True,hue="gender", ax=ax)
  plt.tight_layout()

# for some reason, i cannot get the violin plot to work with credit_card_default

# sns.swarmplot(data=X, x=col, y=X["age"], palette="Set2", hue="gender", ax=ax)

"""### Thoughts:
- most of the correlation thoughts were expressed above in section 2
- some interesting things from the violin plots, mostly when it comes to jobs though.  You can see men are able to start as accountants as an earlier age, whereas women start HR at a younger age than men.
- You also see a lot of lower skilled jobs have higher participation at younger ages and very old ages... whereas the higher skilled jobs tend to build early and mid and then tail off later
"""

df.columns

"""# (7.) Feature Selection (Part 1)
most of the feature selection has been done.
But we need to look into dropping occupation_type as well as we will drop either credit limit or net yearly income.

I think since credit info appears in a few other categories, we will drop credit limit.  We had to drop 1 do to their extremely high correlation.  Credit limit is essentially just measuring your net yearly income, so we do not want to count this twice.
"""

X.head()

X.shape

y.shape

X.drop(['credit_limit'], axis=1,inplace=True)

X.shape

"""# (9.) Encoding Data

When deciding between get_dummies() and DictVectorizer(), consider the size of your dataset, the number of distinct values in your categorical variables, and whether you need to use scikit-learn for other tasks like model training. If you are working with a small dataset and using pandas, get_dummies() may be a convenient choice. If you are working with a large dataset and need to use scikit-learn, DictVectorizer() may be a more efficient choice.

So we are going to use the Dictionary Vectorizer

## Splitting X data into numeric and categorical
"""

X.shape

catDF = X.select_dtypes(exclude=numerics)
numDF = X.select_dtypes(include=numerics)

# saving a copy of X, before its vectorized
X_copy = pd.concat([catDF,numDF],axis=1)

numDF.shape

"""## Using Dictionary Vectorizer"""

from sklearn.feature_extraction import DictVectorizer

# Create a dictionary from catDF
cat_dict = catDF.to_dict('records')

# Create an instance of DictVectorizer and fit_transform on the dictionary
vectorizer = DictVectorizer(sparse=False)
cat_vector = vectorizer.fit_transform(cat_dict)

# Create a new dataframe from the vectorized categorical variables
catDF_vectorized = pd.DataFrame(cat_vector, columns=vectorizer.get_feature_names())

catDF_vectorized.head()

# making the new column names look better.  replacing = with _ and taking away CAPS
catDF_vectorized.columns = [col.replace('=', '_').lower() for col in catDF_vectorized.columns]

catDF_vectorized.head()

catDF_vectorized.shape

numDF.shape

catDF_vectorized = catDF_vectorized.reset_index(drop=True)
numDF = numDF.reset_index(drop=True)
X = pd.concat([catDF_vectorized, numDF], axis=1)

X.shape

"""### Thoughts:
Basically, we decided between get dummies and dictionary vectorizer as these are the methods I know best.  We chose dictionary vectorizer because our dataset is somewhat larger.  We went from 14 features, to 35. So 21 new ones.

# (8.) Feature Importance

I felt like it made more sense to do feature importance after OHE.
When you perform feature importance, you are trying to determine which features are most important in predicting the outcome variable. If you perform feature importance before one-hot encoding, you will be treating categorical variables as numerical variables, which could result in incorrect feature importance scores. Therefore, it is recommended to perform one-hot encoding before feature importance to ensure that the feature importance scores reflect the importance of the individual categories within the categorical variable.

## Random Forest Method
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2, random_state=22, stratify=y) # stratify based on the y column

# First we build and train our Random Forest Model 
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(max_depth=5, random_state=42, n_estimators = 300).fit(X_train, y_train)
rf.feature_importances_
# create a new DataFrame with feature importances and column names
feature_importances = pd.DataFrame({'feature': X.columns, 'importance': rf.feature_importances_})

# sort the features by importance
feature_importances = feature_importances.sort_values('importance', ascending=False)

# round the importance column to 5 decimal places
feature_importances['importance'] = feature_importances['importance'].round(5)

# print the feature importances
print(feature_importances)

"""### Quick thoughts:
Occupation looking not all that important....
also owning as house as suspected, not that important

## Permutation Importance Method
"""

from sklearn.inspection import permutation_importance
r = permutation_importance(rf, X_test, y_test,
                           n_repeats=10,
                           random_state=0)
perm = pd.DataFrame(columns=['AVG_Importance'], index=[i for i in X_train.columns])
perm['AVG_Importance'] = r.importances_mean

perm['AVG_Importance'] = round(perm['AVG_Importance'], 6)
print(perm.to_string())

from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier()
kn.fit(X,y)

results = permutation_importance(kn, X_test, y_test, n_repeats=10, random_state=0)

from matplotlib import pyplot

# get importance
importance = feature_importances['importance']
importance = results.importances_mean
importance = np.sort(importance)

# get feature names
feature_names = feature_importances['feature']

# summarize feature importance
for i, v in enumerate(importance):
    print('Feature: {} - Name: {} - Score: {}'.format(i, feature_names[i], v))

# plot feature importance
pyplot.bar([x for x in range(len(importance))], importance)
pyplot.xticks(range(len(importance)), feature_names, rotation=90)
pyplot.show()

"""### Quick Thoughts:
once again occupation not that important...
Also this says gender and owning a car/house not imporant

## Linear Regression Coefficients Method
"""

# Coefficients
model = LogisticRegression(max_iter=1000)
model.fit(X,y)
importance = model.coef_[0]
importance = np.sort(importance)
importance
importance = pd.DataFrame({'feature': X.columns, 'importance': importance})
importance = importance.sort_values('importance', ascending=False)

# create a chart of feature importances
plt.figure(figsize=(10,5))
plt.bar(importance['feature'], importance['importance'])
plt.xticks(rotation=90)
plt.show()

"""### Thoughts: Feature Importance
So, the through line with all 3 methods seems to be that owning a house, and most occupations do not matter.

I'm going to use Random Forest method as my basis.
5              occupation_type_core staff     0.00022
4           occupation_type_cooking staff     0.00012
is a nice jump. So we will eliminate all those from cooking_staff and before.

# (7.) Feature Selection (Part 2.)

So we are going to eliminate the least useful occupation features, as well as owning a house.
"""

X.shape

# number of bottom columns to drop
n = 17

# extract the column names of the bottom n features
bottom_features = feature_importances.tail(n)['feature'].tolist()
bottom_features
# drop the bottom n features from X
X = X.drop(bottom_features, axis=1)

X.shape

X.head()

"""## Thoughts:
So we basically eliminated just under half of our features.  17 eliminated out of 35.  18 LEFT

# (10.) Scaling

 - Min max scaler is better for unevenly distributed data
 - Standardscaler is better for normalized data
 - RobustScaler: This method is useful when the dataset has outliers that can affect the mean and standard deviation. similar to standard scaler.
 - Max absolute scaler - from -1 to 1, better with data that has negative quantities

 So earlier, I noticed some of the values were not as evenly distributed as I would like.  I think it will be best to use the min max scaler.
"""

# scale the data
from sklearn import preprocessing
minmaxScaler = preprocessing.MinMaxScaler()

X_tran = pd.DataFrame(minmaxScaler.fit_transform(X))
X_tran.head()

# (10.) One More Time Through Algorithm Harness

X_train, X_test, y_train, y_test = train_test_split(X_tran, y, test_size=0.2, random_state=22, stratify=y)

# Compare Algorithms
from sklearn.metrics import roc_auc_score
from time import time
from sklearn.metrics import explained_variance_score,mean_absolute_error,r2_score
from pandas import read_csv
from matplotlib import pyplot
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from xgboost import XGBClassifier
models = []
models.append(('LR', LogisticRegression(solver='liblinear')))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('XGB', XGBClassifier()))
# models.append(('SVM', SVC(gamma='auto')))
# evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
    start = time()
    kfold = KFold(n_splits=10, random_state=7, shuffle=True)
    model.fit(X_train, y_train)
    train_time = time() - start
    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)
    predict_time = time()-start 
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    #y_pred = model.predict_proba(X_train)[:, 1]
    #auc = roc_auc_score(y_train, y_pred)
    print(msg)
    print("Score for each of the 10 K-fold tests: ",cv_results)
    print(model)
    print("\tTraining time: %0.3fs" % train_time)
    print("\tPrediction time: %0.3fs" % predict_time)
    #y_pred = model.predict(X_test)
    #print("\tExplained variance:", explained_variance_score(y_test, y_pred))
    print()
    
    
    
# boxplot algorithm comparison
fig = pyplot.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
pyplot.show()

"""## LDA look again"""

y_pred = model.predict(X_test)
confusion_matrix(y_test, y_pred)

print (classification_report (y_test, y_pred))

"""# Final Thoughts:

So, if you look of all the models we ran through the harness, they all got more accurate, aside from XGBoost and Decision Tree.  But those only went down slightly. 

- Models before Wrangling
-- LR: 95.4
-- LDA: 97.65
-- KNN: 91.37
-- CART: 97.37
-- NB: 91.87 
-- XGB: 97.83

- Post Wrangling
-- LR: 97.68
-- LDA: 97.66
-- KNN: 97.13
-- CART: 97.03
-- NB: 97.66
-- XGB: 97.44

Also for LDA, which we looked at more closely, the accuracy overall slightly improved but the recall of the Yes class went from 69% to 78% while the f1 score actually went up from .82 to .83 for Yes and stayed at .99 for No. So we didn't sacrifice to get this increased recall.

This is without tuning or dealing with the imbalance in our dataset (via under or over sampling.

We were able to eliminate started with 18 features.  We were able to eliminate a few right off the bat due to unimportance as well as multicolinearity.  Then our dataset grew to 35 features as a result of encoding our categorical variables.  From feature importance we trimmed that back down to 18 features, Right back where we started from.  So with the same amount of features that the dataset originally had we are able to pretty much improve accuracy across the board using our rangling techniques.

# Future Work:

1. Employing Under or Over sampling to deal with the fact that our dataset is heavily imbalanced.  In my past work using smote/msmote etc has resulted in the biggest gains for model improvement.  Considering how accurate our model already is, it would be interesting to see how much more it would improve from just that process.
2. Tuning then either for accuracy or more likely for recall of the yes class would also help.  Most likely I would chose to tune the hyper parameters for XGBoost as the seems like the most accurate model.  We should be able to potentially squeeze out a bit more if we tune the hyperparameters as well.
"""

